{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ˆ Project: Big Brother (Dual-LSTM Trading System)\n",
        "**Created by:** Daryl James Padogdog  \n",
        "**Date:** December 2025  \n",
        "**Framework:** TensorFlow/Keras & Python\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Why I Built This: \"Stop Guessing, Start Calculating\"\n",
        "\n",
        "I developed this system because standard algorithmic trading often fails due to a single problem: **Noise.**\n",
        "\n",
        "If you train an AI on **5-minute candles**, it panics at every small drop (Overfitting).  \n",
        "If you train it on **1-hour candles**, it reacts too slowly and misses the entry (Lagging).\n",
        "\n",
        "I realized that a single model cannot effectively handle both the *Trend* and the *Entry*. So, I split the \"brain\" of the system into two distinct neural networks.\n",
        "\n",
        "### âš™ï¸ The Architecture\n",
        "\n",
        "1.  **ðŸ‘ï¸ Big Brother (The Strategist)**\n",
        "    * **Input:** 1-Hour Data (168-hour lookback).\n",
        "    * **Role:** Analyzes the macro trend. It ignores short-term volatility to determine if the market is **Bullish** or **Bearish**.\n",
        "    * **Logic:** Acts as the primary filter. If Big Brother says \"Bearish,\" Little Brother is forbidden from buying.\n",
        "\n",
        "2.  **ðŸŽ¯ Little Brother (The Sniper)**\n",
        "    * **Input:** 5-Minute Data (48-period lookback).\n",
        "    * **Role:** Finds the precise entry point within the trend.\n",
        "    * **Logic:** Hunts for micro-reversals and price anomalies, aligned with the macro trend.\n",
        "\n",
        "3.  **ðŸ›¡ï¸ Risk & Math Layer**\n",
        "    * **Dynamic Bias Correction:** A rolling window that calculates the AI's recent error rate and adjusts predictions in real-time.\n",
        "    * **Monte Carlo Simulation:** A projection engine that bootstraps historical performance to estimate wealth generation for 2026.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“‹ Notebook Roadmap\n",
        "This notebook is structured into four distinct phases:\n",
        "1.  **Training Phase:** Building and saving the `big_brother` (1H) and `little_brother` (5M) models.\n",
        "2.  **Hybrid Backtest:** Running the \"Dual-Timeframe\" logic on test data to verify win rates.\n",
        "3.  **2025 Prediction:** AI analysis of current market data (Test Set) to visualize performance.\n",
        "4.  **Wealth Simulation:** A Monte Carlo projection of account growth over the next 2 years.\n",
        "\n",
        "> *Note: Ensure `TRAIN_1h.csv`, `TRAIN_5m.csv`, `TEST_1h.csv`, and `TEST_5m.csv` are uploaded before running.*"
      ],
      "metadata": {
        "id": "Saw9sNR_SqX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIRST WE TRAIN THE BIG BROTHER\n",
        "**big_brother_v2.keras**"
      ],
      "metadata": {
        "id": "WSSGXH_OVG6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# TRAIN BIG BROTHER MODEL (1H Data)\n",
        "# ===========================================\n",
        "# Run this FIRST in a fresh Colab session\n",
        "# After training completes, download 'big_brother_v2.keras'\n",
        "# ===========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    LSTM, Dense, Dropout, Bidirectional,\n",
        "    Input, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "GTMUpuVTSpGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# CONFIGURATION\n",
        "# ===========================================\n",
        "TRAIN_FILE = 'TRAIN_1h.csv'\n",
        "LOOKBACK = 168  # 1 week of hourly data (7*24)\n",
        "MODEL_NAME = 'big_brother_v2.keras'\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'Returns', 'EMA_20', 'EMA_50', 'EMA_Diff',\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
        "    'BB_Width', 'BB_Position', 'ATR', 'Volatility'\n",
        "]"
      ],
      "metadata": {
        "id": "oCP4WuhxVaa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# TECHNICAL INDICATORS\n",
        "# ===========================================\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd - signal_line\n",
        "    return macd, signal_line, histogram\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
        "    middle = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "    upper = middle + (std * std_dev)\n",
        "    lower = middle - (std * std_dev)\n",
        "    return upper, middle, lower\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    df['EMA_Diff'] = df['EMA_20'] - df['EMA_50']\n",
        "    df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['Close'])\n",
        "    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df['Close'])\n",
        "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'])\n",
        "    df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "    return df"
      ],
      "metadata": {
        "id": "KtjcC7V8VemA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# DATA LOADING\n",
        "# ===========================================\n",
        "def load_and_preprocess(filepath):\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    df = pd.read_csv(filepath, sep=';')\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d %H:%M')\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    df = add_technical_indicators(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Convert to float32 to save memory\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "\n",
        "    print(f\"Loaded {len(df)} rows\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "mYo-emnVViTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# MODEL\n",
        "# ===========================================\n",
        "def build_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "y6R-uHtmVmag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# MAIN\n",
        "# ===========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  BIG BROTHER TRAINING (1H Trend Model)\")\n",
        "    print(\"  Created by: Daryl James Padogdog\")\n",
        "    print(\"  Purpose:    Analyzes the 1-Hour timeframe to determine the\")\n",
        "    print(\"              Broad Market Trend (Bullish/Bearish).\")\n",
        "    print(\"              This acts as the primary filter for the system.\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n[INFO] Starting process...\")\n",
        "\n",
        "    # Load data\n",
        "    df = load_and_preprocess(TRAIN_FILE)\n",
        "\n",
        "    # Normalize\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[FEATURE_COLS] = scaler.fit_transform(df[FEATURE_COLS]).astype('float32')\n",
        "\n",
        "    # Create datasets using tf.data (Streaming)\n",
        "    # -----------------------------------------\n",
        "    # Split data indices first (prevent data leakage)\n",
        "    n = len(df_scaled)\n",
        "    train_n = int(n * 0.85)\n",
        "\n",
        "    data_array = df_scaled[FEATURE_COLS].values\n",
        "    target_array = df_scaled['Close'].values\n",
        "\n",
        "    # Train Dataset\n",
        "    train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data_array[:-LOOKBACK], # slice end to match target\n",
        "        targets=target_array[LOOKBACK:], # offset targets by lookback\n",
        "        sequence_length=LOOKBACK,\n",
        "        sequence_stride=1,\n",
        "        shuffle=True,\n",
        "        batch_size=64,\n",
        "        start_index=0,\n",
        "        end_index=train_n\n",
        "    )\n",
        "\n",
        "    # Validation Dataset\n",
        "    val_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data_array[:-LOOKBACK],\n",
        "        targets=target_array[LOOKBACK:],\n",
        "        sequence_length=LOOKBACK,\n",
        "        sequence_stride=1,\n",
        "        shuffle=False,\n",
        "        batch_size=64,\n",
        "        start_index=train_n\n",
        "    )\n",
        "\n",
        "    print(\"âœ“ Datasets created (streaming)\")\n",
        "\n",
        "    # Free dataframe memory\n",
        "    del df, df_scaled\n",
        "    gc.collect()\n",
        "\n",
        "    # Build and train\n",
        "    model = build_model((LOOKBACK, len(FEATURE_COLS)))\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "    ]\n",
        "\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=100, callbacks=callbacks)\n",
        "\n",
        "    # Save\n",
        "    model.save(MODEL_NAME)\n",
        "    print(f\"\\nâœ“ Model saved as '{MODEL_NAME}'\")\n",
        "    print(\"Download this file and keep it safe!\")\n"
      ],
      "metadata": {
        "id": "S6R3Gjy4Vwbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOW WE WILL TRAIN THE LIITLE BROTHER\n",
        "**little_brother_v2.keras**"
      ],
      "metadata": {
        "id": "EWFq_GW9U2WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# TRAIN LITTLE BROTHER MODEL (5M Data)\n",
        "# ===========================================\n",
        "# Run this SECOND in a fresh Colab session\n",
        "# (After Big Brother is trained and downloaded)\n",
        "# After training completes, download:\n",
        "#   - 'little_brother_v2.keras'\n",
        "#   - 'scaler_small.pkl' (needed for predictions)\n",
        "# ===========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    LSTM, Dense, Dropout, Bidirectional,\n",
        "    Input, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "RPipq8a7Uwkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# CONFIGURATION\n",
        "# ===========================================\n",
        "TRAIN_FILE = 'TRAIN_5m.csv'\n",
        "LOOKBACK = 48  # 4 hours of 5-min data\n",
        "MODEL_NAME = 'little_brother_v2.keras'\n",
        "SCALER_NAME = 'scaler_small.pkl'\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'Returns', 'EMA_20', 'EMA_50', 'EMA_Diff',\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
        "    'BB_Width', 'BB_Position', 'ATR', 'Volatility'\n",
        "]"
      ],
      "metadata": {
        "id": "raTHgGyoV78k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# TECHNICAL INDICATORS\n",
        "# ===========================================\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd - signal_line\n",
        "    return macd, signal_line, histogram\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
        "    middle = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "    upper = middle + (std * std_dev)\n",
        "    lower = middle - (std * std_dev)\n",
        "    return upper, middle, lower\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    df['EMA_Diff'] = df['EMA_20'] - df['EMA_50']\n",
        "    df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['Close'])\n",
        "    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df['Close'])\n",
        "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'])\n",
        "    df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "uzIAOCkXV-Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# DATA LOADING\n",
        "# ===========================================\n",
        "def load_and_preprocess(filepath):\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    df = pd.read_csv(filepath, sep=';')\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d %H:%M')\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    df = add_technical_indicators(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Convert to float32 to save memory\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "\n",
        "    print(f\"Loaded {len(df)} rows\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "bAdOtJiTWB3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# MODEL\n",
        "# ===========================================\n",
        "def build_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "UEpjHxxoWEsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# MAIN\n",
        "# ===========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  LITTLE BROTHER TRAINING (5M Entry Model)\")\n",
        "    print(\"  Created by: Daryl James Padogdog\")\n",
        "    print(\"  Purpose:    Analyzes the 5-Minute timeframe to find precise\")\n",
        "    print(\"              entry points aligned with the Big Brother trend.\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n[INFO] Starting process...\")\n",
        "\n",
        "    # Load data\n",
        "    df = load_and_preprocess(TRAIN_FILE)\n",
        "\n",
        "    # Normalize and SAVE the scaler (needed for predictions later)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[FEATURE_COLS] = scaler.fit_transform(df[FEATURE_COLS]).astype('float32')\n",
        "\n",
        "    # Save scaler for later use\n",
        "    with open(SCALER_NAME, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    print(f\"âœ“ Scaler saved as '{SCALER_NAME}'\")\n",
        "\n",
        "    # Also save the close scaler\n",
        "    close_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    close_scaler.fit(df[['Close']])\n",
        "    with open('close_scaler.pkl', 'wb') as f:\n",
        "        pickle.dump(close_scaler, f)\n",
        "    print(\"âœ“ Close scaler saved as 'close_scaler.pkl'\")\n",
        "\n",
        "    # Create datasets using tf.data (Streaming)\n",
        "    # -----------------------------------------\n",
        "    # Split data indices first (prevent data leakage)\n",
        "    n = len(df_scaled)\n",
        "    train_n = int(n * 0.85)\n",
        "\n",
        "    data_array = df_scaled[FEATURE_COLS].values\n",
        "    target_array = df_scaled['Close'].values\n",
        "\n",
        "    # Train Dataset\n",
        "    train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data_array[:-LOOKBACK], # slice end to match target\n",
        "        targets=target_array[LOOKBACK:], # offset targets by lookback\n",
        "        sequence_length=LOOKBACK,\n",
        "        sequence_stride=1,\n",
        "        shuffle=True,\n",
        "        batch_size=64,\n",
        "        start_index=0,\n",
        "        end_index=train_n\n",
        "    )\n",
        "\n",
        "    # Validation Dataset\n",
        "    val_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data_array[:-LOOKBACK],\n",
        "        targets=target_array[LOOKBACK:],\n",
        "        sequence_length=LOOKBACK,\n",
        "        sequence_stride=1,\n",
        "        shuffle=False,\n",
        "        batch_size=64,\n",
        "        start_index=train_n\n",
        "    )\n",
        "\n",
        "    print(\"âœ“ Datasets created (streaming)\")\n",
        "\n",
        "    # Free dataframe memory (the arrays are now referenced by the dataset)\n",
        "    del df, df_scaled\n",
        "    gc.collect()\n",
        "\n",
        "    # Build and train\n",
        "    model = build_model((LOOKBACK, len(FEATURE_COLS)))\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "    ]\n",
        "\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=100, callbacks=callbacks)\n",
        "\n",
        "    # Save\n",
        "    model.save(MODEL_NAME)\n",
        "    print(f\"\\nâœ“ Model saved as '{MODEL_NAME}'\")\n",
        "    print(\"\\nDownload these files:\")\n",
        "    print(f\"  - {MODEL_NAME}\")\n",
        "    print(f\"  - {SCALER_NAME}\")\n",
        "    print(\"  - close_scaler.pkl\")"
      ],
      "metadata": {
        "id": "kiSGH1yuWHjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOU CAN NOW ADD A ALGORTIHM TO THE MODELS WHICH YOU LIKE OR TEST IT USING THIS"
      ],
      "metadata": {
        "id": "5PqnMM2cWNN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============\n",
        "# HYBRID DUAL-TIMEFRAME DEMO\n",
        "# =============\n",
        "# Uses the same logic as Trading_AI_model.ipynb:\n",
        "# - Big Brother: Determines overall market trend (BULLISH/BEARISH)\n",
        "# - Little Brother: Confirms precise trade entries\n",
        "# - Only trades when BOTH timeframes agree\n",
        "# ===========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# =============\n",
        "# CONFIGURATION\n",
        "# =============\n",
        "BIG_BROTHER_MODEL = 'big_brother_v2.keras'\n",
        "LITTLE_BROTHER_MODEL = 'little_brother_v2.keras'\n",
        "SCALER_SMALL = 'scaler_small.pkl'\n",
        "CLOSE_SCALER = 'close_scaler.pkl'\n",
        "\n",
        "TEST_5M = 'TEST_5m.csv'\n",
        "TEST_1H = 'TEST_1h.csv'\n",
        "\n",
        "LOOKBACK_BIG = 168   # 1 week of hourly data\n",
        "LOOKBACK_SMALL = 48  # 4 hours of 5-min data\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'Returns', 'EMA_20', 'EMA_50', 'EMA_Diff',\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
        "    'BB_Width', 'BB_Position', 'ATR', 'Volatility'\n",
        "]\n",
        "\n",
        "# =============\n",
        "# TECHNICAL INDICATORS\n",
        "# =============\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd - signal_line\n",
        "    return macd, signal_line, histogram\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
        "    middle = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "    upper = middle + (std * std_dev)\n",
        "    lower = middle - (std * std_dev)\n",
        "    return upper, middle, lower\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    df['EMA_Diff'] = df['EMA_20'] - df['EMA_50']\n",
        "    df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['Close'])\n",
        "    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df['Close'])\n",
        "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'])\n",
        "    df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "    return df\n",
        "\n",
        "def load_and_preprocess(filepath):\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    df = pd.read_csv(filepath, sep=';')\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d %H:%M')\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    df = add_technical_indicators(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "    print(f\"Loaded {len(df)} rows\")\n",
        "    return df\n",
        "\n",
        "# =============\n",
        "# HYBRID BACKTEST LOGIC (Optimized)\n",
        "# =============\n",
        "def hybrid_backtest(df_5m, little_predictions, df_1h, big_predictions, lookback_5m, lookback_1h, initial_capital=200.0):\n",
        "    \"\"\"\n",
        "    Backtest with PHASE 2 Optimizations & Paper Trading:\n",
        "    1. Dynamic Bias Correction.\n",
        "    2. Stricter Threshold & EMA Filter.\n",
        "    3. Paper Trading ($200 Start).\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'total_signals': 0,\n",
        "        'confirmed_trades': 0,\n",
        "        'wins': 0,\n",
        "        'losses': 0,\n",
        "        'buy_signals': 0,\n",
        "        'sell_signals': 0,\n",
        "        'wait_signals': 0,\n",
        "        'filtered_signals': 0,\n",
        "        'initial_balance': initial_capital,\n",
        "        'final_balance': initial_capital,\n",
        "        'net_profit': 0.0\n",
        "    }\n",
        "\n",
        "    trades = []\n",
        "    equity_curve = [initial_capital]\n",
        "    signals_log = []\n",
        "\n",
        "    # --- Bias Correction Setup ---\n",
        "    bias_window = []\n",
        "    BIAS_WINDOW_SIZE = 50\n",
        "\n",
        "    # --- Filters (PHASE 2) ---\n",
        "    RSI_BUY_MAX = 70\n",
        "    RSI_SELL_MIN = 30\n",
        "    MIN_THRESHOLD = 0.0015\n",
        "\n",
        "    # Align Data\n",
        "    df_5m_subset = df_5m.iloc[lookback_5m-1:lookback_5m-1+len(little_predictions)].copy()\n",
        "    df_5m_subset['Predicted_Raw'] = little_predictions\n",
        "\n",
        "    df_1h_subset = df_1h.iloc[lookback_1h-1:lookback_1h-1+len(big_predictions)].copy()\n",
        "    df_1h_subset['Predicted_Raw'] = big_predictions\n",
        "\n",
        "    # 1H Mapping (Added EMAs for Trend Filter)\n",
        "    df_1h_subset['Hour'] = df_1h_subset['Date'].dt.floor('H')\n",
        "    hourly_trend = df_1h_subset.set_index('Hour')[\n",
        "        ['Close', 'Predicted_Raw', 'EMA_20', 'EMA_50']\n",
        "    ].to_dict('index')\n",
        "\n",
        "    current_balance = initial_capital\n",
        "\n",
        "    # --- MAIN LOOP ---\n",
        "    for i in range(len(df_5m_subset) - 1):\n",
        "        row = df_5m_subset.iloc[i]\n",
        "        current_price = row['Close']\n",
        "        actual_next = df_5m_subset.iloc[i + 1]['Close']\n",
        "\n",
        "        # 1. Update Bias\n",
        "        raw_pred_5m = row['Predicted_Raw']\n",
        "        current_bias = current_price - raw_pred_5m\n",
        "        bias_window.append(current_bias)\n",
        "        if len(bias_window) > BIAS_WINDOW_SIZE:\n",
        "            bias_window.pop(0)\n",
        "        avg_bias = sum(bias_window) / len(bias_window)\n",
        "\n",
        "        pred_5m_corrected = raw_pred_5m + avg_bias\n",
        "\n",
        "        # 2. Big Brother Trend (STRICTER EMA FILTER)\n",
        "        hour_key = row['Date'].floor('H')\n",
        "        trend = \"NEUTRAL\"\n",
        "\n",
        "        if hour_key in hourly_trend:\n",
        "            h_data = hourly_trend[hour_key]\n",
        "            pred_bullish = (h_data['Predicted_Raw'] - h_data['Close']) > -avg_bias\n",
        "            ema_bullish = (h_data['Close'] > h_data['EMA_50']) and (h_data['EMA_20'] > h_data['EMA_50'])\n",
        "            ema_bearish = (h_data['Close'] < h_data['EMA_50']) and (h_data['EMA_20'] < h_data['EMA_50'])\n",
        "\n",
        "            if pred_bullish and ema_bullish:\n",
        "                trend = \"BULLISH\"\n",
        "            elif not pred_bullish and ema_bearish:\n",
        "                trend = \"BEARISH\"\n",
        "\n",
        "        # 3. Little Brother Entry\n",
        "        pred_change = (pred_5m_corrected - current_price) / current_price\n",
        "\n",
        "        entry_signal = \"WAIT\"\n",
        "        if pred_change > MIN_THRESHOLD:\n",
        "            entry_signal = \"BUY\"\n",
        "        elif pred_change < -MIN_THRESHOLD:\n",
        "            entry_signal = \"SELL\"\n",
        "\n",
        "        # 4. RSI Filter\n",
        "        if entry_signal == \"BUY\" and row['RSI'] > RSI_BUY_MAX:\n",
        "            entry_signal = \"WAIT\"\n",
        "            results['filtered_signals'] += 1\n",
        "        if entry_signal == \"SELL\" and row['RSI'] < RSI_SELL_MIN:\n",
        "            entry_signal = \"WAIT\"\n",
        "            results['filtered_signals'] += 1\n",
        "\n",
        "        # === FINAL DECISION ===\n",
        "        results['total_signals'] += 1\n",
        "        final_signal = \"WAIT\"\n",
        "        trade_return = 0.0\n",
        "\n",
        "        if trend == \"BULLISH\" and entry_signal == \"BUY\":\n",
        "            final_signal = \"BUY NOW\"\n",
        "            results['buy_signals'] += 1\n",
        "            results['confirmed_trades'] += 1\n",
        "\n",
        "            trade_return = (actual_next - current_price) / current_price\n",
        "            trades.append(trade_return)\n",
        "\n",
        "            if actual_next > current_price:\n",
        "                results['wins'] += 1\n",
        "            else:\n",
        "                results['losses'] += 1\n",
        "\n",
        "        elif trend == \"BEARISH\" and entry_signal == \"SELL\":\n",
        "            final_signal = \"SELL NOW\"\n",
        "            results['sell_signals'] += 1\n",
        "            results['confirmed_trades'] += 1\n",
        "\n",
        "            trade_return = (current_price - actual_next) / current_price\n",
        "            trades.append(trade_return)\n",
        "\n",
        "            if actual_next < current_price:\n",
        "                results['wins'] += 1\n",
        "            else:\n",
        "                results['losses'] += 1\n",
        "\n",
        "        elif entry_signal != \"WAIT\":\n",
        "            results['wait_signals'] += 1\n",
        "\n",
        "        # Update Paper Trading Balance\n",
        "        if trade_return != 0:\n",
        "            current_balance = current_balance * (1 + trade_return)\n",
        "\n",
        "        equity_curve.append(current_balance)\n",
        "\n",
        "        signals_log.append({\n",
        "            'date': row['Date'],\n",
        "            'price': current_price,\n",
        "            'pred': pred_5m_corrected,\n",
        "            'trend': trend,\n",
        "            'final': final_signal\n",
        "        })\n",
        "\n",
        "    # Metrics\n",
        "    results['final_balance'] = current_balance\n",
        "    results['net_profit'] = current_balance - initial_capital\n",
        "\n",
        "    if results['confirmed_trades'] > 0:\n",
        "        results['win_rate'] = (results['wins'] / results['confirmed_trades']) * 100\n",
        "        returns_series = pd.Series(trades)\n",
        "        cumulative_returns = (1 + returns_series).cumprod()\n",
        "        results['total_return'] = (cumulative_returns.iloc[-1] - 1) * 100\n",
        "        results['sharpe_ratio'] = np.sqrt(252 * 288) * returns_series.mean() / returns_series.std() if returns_series.std() > 0 else 0\n",
        "    else:\n",
        "        results['win_rate'] = 0\n",
        "        results['total_return'] = 0\n",
        "        results['sharpe_ratio'] = 0\n",
        "\n",
        "    return results, trades, signals_log, avg_bias, equity_curve\n",
        "\n",
        "# =============\n",
        "# MAIN\n",
        "# =============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  HYBRID DUAL-TIMEFRAME DEMO (Phase 2 Optimized)\")\n",
        "    print(\"  Stricter Filters | EMA Alignment | Bias Correction\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ... (Model Loading Logic - Condensed for brevity, functionality maintained)\n",
        "    # ===== LOAD MODELS =====\n",
        "    print(\"\\n[1/5] Loading models...\")\n",
        "    try:\n",
        "        big_model = load_model(BIG_BROTHER_MODEL)\n",
        "        print(f\"  âœ“ Big Brother (1H) loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Big Brother not found: {e}\")\n",
        "        big_model = None\n",
        "\n",
        "    try:\n",
        "        little_model = load_model(LITTLE_BROTHER_MODEL)\n",
        "        print(f\"  âœ“ Little Brother (5M) loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Little Brother not found: {e}\")\n",
        "        little_model = None\n",
        "\n",
        "    if not big_model or not little_model:\n",
        "        exit()\n",
        "\n",
        "    with open(SCALER_SMALL, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(CLOSE_SCALER, 'rb') as f:\n",
        "        close_scaler = pickle.load(f)\n",
        "    print(\"  âœ“ Scalers loaded\")\n",
        "\n",
        "    # ===== LOAD DATA =====\n",
        "    print(\"\\n[2/5] Loading 2025 test data...\")\n",
        "    df_5m = load_and_preprocess(TEST_5M)\n",
        "    try:\n",
        "        df_1h = load_and_preprocess(TEST_1H)\n",
        "    except:\n",
        "        print(\" Using resampled 1H data...\")\n",
        "        pass\n",
        "\n",
        "    # ===== RUN PREDICTIONS =====\n",
        "    print(\"\\n[3/5] Running predictions...\")\n",
        "\n",
        "    # Little Brother (5M)\n",
        "    print(\"  Running Little Brother (5M)...\")\n",
        "    df_5m_scaled = df_5m.copy()\n",
        "    df_5m_scaled[FEATURE_COLS] = scaler.transform(df_5m[FEATURE_COLS])\n",
        "\n",
        "    predict_5m = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=df_5m_scaled[FEATURE_COLS].values, targets=None,\n",
        "        sequence_length=LOOKBACK_SMALL, batch_size=256, shuffle=False\n",
        "    )\n",
        "    pred_5m_scaled = little_model.predict(predict_5m, verbose=0)\n",
        "    little_predictions = close_scaler.inverse_transform(pred_5m_scaled).flatten()\n",
        "    print(f\"  âœ“ {len(little_predictions)} predictions\")\n",
        "\n",
        "    # Big Brother (1H)\n",
        "    print(\"  Running Big Brother (1H)...\")\n",
        "    df_1h_scaled = df_1h.copy()\n",
        "    df_1h_scaled[FEATURE_COLS] = scaler.transform(df_1h[FEATURE_COLS])\n",
        "\n",
        "    predict_1h = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=df_1h_scaled[FEATURE_COLS].values, targets=None,\n",
        "        sequence_length=LOOKBACK_BIG, batch_size=64, shuffle=False\n",
        "    )\n",
        "    pred_1h_scaled = big_model.predict(predict_1h, verbose=0)\n",
        "    big_predictions = close_scaler.inverse_transform(pred_1h_scaled).flatten()\n",
        "    print(f\"  âœ“ {len(big_predictions)} predictions\")\n",
        "\n",
        "    # ===== HYBRID BACKTEST =====\n",
        "    print(\"\\n[4/5] Running Phase 2 Optimization & Simulation...\")\n",
        "\n",
        "    metrics, trades, signals, final_bias, equity_curve = hybrid_backtest(\n",
        "        df_5m, little_predictions,\n",
        "        df_1h, big_predictions,\n",
        "        LOOKBACK_SMALL, LOOKBACK_BIG,\n",
        "        initial_capital=200.0\n",
        "    )\n",
        "\n",
        "    # ===== DISPLAY RESULTS =====\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  HYBRID BACKTEST RESULTS (OPTIMIZED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"\\n  ðŸ“Š Signal Analysis:\")\n",
        "    print(f\"     Total Candles:      {metrics['total_signals']}\")\n",
        "    print(f\"     Confirmed Trades:   {metrics['confirmed_trades']} (Reduced Noise)\")\n",
        "    print(f\"     Filtered Signals:   {metrics['filtered_signals']}\")\n",
        "\n",
        "    print(f\"\\n  ðŸ’° Trading Performance:\")\n",
        "    print(f\"     BUY Trades:         {metrics['buy_signals']}\")\n",
        "    print(f\"     SELL Trades:        {metrics['sell_signals']}\")\n",
        "    print(f\"     Wins:               {metrics['wins']}\")\n",
        "    print(f\"     Losses:             {metrics['losses']}\")\n",
        "    print(f\"     Win Rate:           {metrics['win_rate']:.2f}%\")\n",
        "    print(f\"     Total Return:       {metrics['total_return']:.2f}%\")\n",
        "\n",
        "    print(f\"\\n  ðŸ’µ Paper Trade Simulation ($200 Start):\")\n",
        "    print(f\"     Initial Balance:    ${metrics['initial_balance']:.2f}\")\n",
        "    print(f\"     Final Balance:      ${metrics['final_balance']:.2f}\")\n",
        "    profit_sign = \"+\" if metrics['net_profit'] >= 0 else \"-\"\n",
        "    print(f\"     Net Profit:         {profit_sign}${abs(metrics['net_profit']):.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  CURRENT TRADING SIGNAL (Bias Corrected)\")\n",
        "    print(\"=\" * 60)\n",
        "    current_price = df_5m['Close'].iloc[-1]\n",
        "\n",
        "    # 5M Prediction logic...\n",
        "    latest_5m = df_5m_scaled.iloc[-LOOKBACK_SMALL:][FEATURE_COLS].values[None, ...]\n",
        "    pred_5m_raw = close_scaler.inverse_transform(little_model.predict(latest_5m, verbose=0))[0][0]\n",
        "    target_5m_corrected = pred_5m_raw + final_bias\n",
        "\n",
        "    # 1H Prediction logic...\n",
        "    latest_1h = df_1h_scaled.iloc[-LOOKBACK_BIG:][FEATURE_COLS].values[None, ...]\n",
        "    pred_1h_raw = close_scaler.inverse_transform(big_model.predict(latest_1h, verbose=0))[0][0]\n",
        "    current_price_1h = df_1h['Close'].iloc[-1]\n",
        "    ema_20_1h = df_1h['EMA_20'].iloc[-1]\n",
        "    ema_50_1h = df_1h['EMA_50'].iloc[-1]\n",
        "\n",
        "    ema_bullish = (current_price_1h > ema_50_1h) and (ema_20_1h > ema_50_1h)\n",
        "    ema_bearish = (current_price_1h < ema_50_1h) and (ema_20_1h < ema_50_1h)\n",
        "    pred_bullish_1h = (pred_1h_raw + final_bias) > current_price_1h\n",
        "\n",
        "    if pred_bullish_1h and ema_bullish:\n",
        "        trend = \"BULLISH (Strong Pattern)\"\n",
        "        trend_icon = \"ðŸŸ¢\"\n",
        "    elif not pred_bullish_1h and ema_bearish:\n",
        "        trend = \"BEARISH (Strong Pattern)\"\n",
        "        trend_icon = \"ðŸ”´\"\n",
        "    else:\n",
        "        trend = \"NEUTRAL (EMA Mismatch)\"\n",
        "        trend_icon = \"âšª\"\n",
        "\n",
        "    print(f\"\\n  [Big Brother] 1H Structure: {trend_icon} {trend}\")\n",
        "    print(f\"     Price: ${current_price_1h:.2f} | EMA50: ${ema_50_1h:.2f}\")\n",
        "\n",
        "    move_pct = (target_5m_corrected - current_price) / current_price\n",
        "    min_thresh = 0.0015\n",
        "    if move_pct > min_thresh: entry = \"BUY\"\n",
        "    elif move_pct < -min_thresh: entry = \"SELL\"\n",
        "    else: entry = \"WAIT (Weak Move)\"\n",
        "\n",
        "    print(f\"\\n  [Little Brother] 5M Signal: {entry}\")\n",
        "    print(f\"     Current: ${current_price:.2f}\")\n",
        "    print(f\"     Target:  ${target_5m_corrected:.2f}\")\n",
        "\n",
        "    print(f\"\\n  {'â”€' * 40}\")\n",
        "    if \"BULLISH\" in trend and entry == \"BUY\": final_sig = \"âœ… BUY NOW\"\n",
        "    elif \"BEARISH\" in trend and entry == \"SELL\": final_sig = \"âœ… SELL NOW\"\n",
        "    else: final_sig = \"â¸ï¸ WAIT / FILTERED\"\n",
        "    print(f\"  >> FINAL SIGNAL: {final_sig}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\n  ðŸ“Š Charts displayed above\")\n",
        "\n",
        "    # Update Fig 1\n",
        "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.08,\n",
        "                       subplot_titles=(f'Hybrid Performance ({metrics[\"win_rate\"]:.1f}%)',\n",
        "                                     f'Account Balance ($200 Start) -> ${metrics[\"final_balance\"]:.2f}',\n",
        "                                     'Signals'))\n",
        "\n",
        "    dates = df_5m['Date'].iloc[LOOKBACK_SMALL-1:LOOKBACK_SMALL-1+len(little_predictions)]\n",
        "    real = df_5m['Close'].iloc[LOOKBACK_SMALL-1:LOOKBACK_SMALL-1+len(little_predictions)]\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=dates, y=real, mode='lines', name='Price', line=dict(color='#00FF00')), row=1, col=1)\n",
        "\n",
        "    # Plot equity curve against trade count\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=list(range(len(equity_curve))), y=equity_curve,\n",
        "                             mode='lines', name='Balance ($)',\n",
        "                             line=dict(color='#00BFFF', width=2), fill='tozeroy'), row=2, col=1)\n",
        "\n",
        "    fig.update_layout(template='plotly_dark', height=900, title='Phase 2 Results + Paper Trade Simulation')\n",
        "    fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7JDdONlRWYuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OR THIS ALSO THIS PREDICTS THE 2025 STOCK MOVEMENT THE METHOS I USED IS TO SEPARATE THE 2025 DATA SET AND CREATE IT AS TEST VALIDATION"
      ],
      "metadata": {
        "id": "33Yj9YXAWhQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============\n",
        "# PREDICT 2025 STOCK MOVEMENT (AI Hybrid)\n",
        "# =============\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# =============\n",
        "# CONFIGURATION\n",
        "# =============\n",
        "# Adjust paths if models are in a specific directory\n",
        "BIG_BROTHER_MODEL = 'big_brother_v2.keras'\n",
        "LITTLE_BROTHER_MODEL = 'little_brother_v2.keras'\n",
        "SCALER_SMALL = 'scaler_small.pkl'\n",
        "CLOSE_SCALER = 'close_scaler.pkl'\n",
        "TEST_5M_FILENAME = 'TEST_5m.csv'\n",
        "TEST_1H_FILENAME = 'TEST_1h.csv'\n",
        "\n",
        "LOOKBACK_BIG = 168   # 1 week of hourly data\n",
        "LOOKBACK_SMALL = 48  # 4 hours of 5-min data\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'Returns', 'EMA_20', 'EMA_50', 'EMA_Diff',\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
        "    'BB_Width', 'BB_Position', 'ATR', 'Volatility'\n",
        "]\n",
        "\n",
        "# =============\n",
        "# TECHNICAL INDICATORS\n",
        "# =============\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd - signal_line\n",
        "    return macd, signal_line, histogram\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
        "    middle = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "    upper = middle + (std * std_dev)\n",
        "    lower = middle - (std * std_dev)\n",
        "    return upper, middle, lower\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    df['EMA_Diff'] = df['EMA_20'] - df['EMA_50']\n",
        "    df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['Close'])\n",
        "    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df['Close'])\n",
        "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'])\n",
        "    df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "    return df\n",
        "\n",
        "def find_file(filename):\n",
        "    \"\"\"Helper to find file in DATA_SET or current directory\"\"\"\n",
        "    # 1. Check DATA_SET/filename\n",
        "    path1 = os.path.join('DATA_SET', filename)\n",
        "    if os.path.exists(path1):\n",
        "        return path1\n",
        "\n",
        "    # 2. Check ./filename (Colab root)\n",
        "    path2 = filename\n",
        "    if os.path.exists(path2):\n",
        "        return path2\n",
        "\n",
        "    return None\n",
        "\n",
        "def load_and_preprocess(filename, filter_year=None):\n",
        "    filepath = find_file(filename)\n",
        "    if not filepath:\n",
        "        print(f\"ERROR: File not found: {filename}\")\n",
        "        print(f\"  Checked: DATA_SET/{filename} and ./{filename}\")\n",
        "        print(f\"  Current Directory: {os.getcwd()}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    df = pd.read_csv(filepath, sep=';')\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d %H:%M')\n",
        "\n",
        "    if filter_year:\n",
        "        print(f\"Filtering for Year {filter_year}...\")\n",
        "        df_filtered = df[df['Date'].dt.year == filter_year]\n",
        "        if df_filtered.empty:\n",
        "            print(f\"WARNING: No data found for year {filter_year} in {filepath}\")\n",
        "            print(f\"  Available Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "            print(\"  -> Proceeding with ALL data in file (assuming this is the intended test set).\")\n",
        "        else:\n",
        "            print(f\"Found {len(df_filtered)} rows for {filter_year}\")\n",
        "            df = df_filtered\n",
        "\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    df = add_technical_indicators(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "    return df\n",
        "\n",
        "# =============\n",
        "# HYBRID BACKTEST LOGIC\n",
        "# =============\n",
        "def hybrid_backtest(df_5m, little_predictions, df_1h, big_predictions, lookback_5m, lookback_1h):\n",
        "    results = {\n",
        "        'total_signals': 0, 'confirmed_trades': 0, 'wins': 0, 'losses': 0,\n",
        "        'buy_signals': 0, 'sell_signals': 0, 'filtered_signals': 0\n",
        "    }\n",
        "\n",
        "    trades = []\n",
        "\n",
        "    # --- Bias Correction Setup ---\n",
        "    bias_window = []\n",
        "    BIAS_WINDOW_SIZE = 50\n",
        "\n",
        "    # --- Filters ---\n",
        "    RSI_BUY_MAX = 70\n",
        "    RSI_SELL_MIN = 30\n",
        "    MIN_THRESHOLD = 0.0015\n",
        "\n",
        "    # Align Data\n",
        "    # Ensure indices match the predictions length\n",
        "    df_5m_subset = df_5m.iloc[lookback_5m-1 : lookback_5m-1+len(little_predictions)].copy()\n",
        "    df_5m_subset['Predicted_Raw'] = little_predictions\n",
        "\n",
        "    df_1h_subset = df_1h.iloc[lookback_1h-1 : lookback_1h-1+len(big_predictions)].copy()\n",
        "    df_1h_subset['Predicted_Raw'] = big_predictions\n",
        "\n",
        "    # 1H Mapping\n",
        "    df_1h_subset['Hour'] = df_1h_subset['Date'].dt.floor('H')\n",
        "    hourly_trend = df_1h_subset.set_index('Hour')[\n",
        "        ['Close', 'Predicted_Raw', 'EMA_20', 'EMA_50']\n",
        "    ].to_dict('index')\n",
        "\n",
        "    print(\"\\nSimulating Trading Logic on Data...\")\n",
        "\n",
        "    for i in range(len(df_5m_subset) - 1):\n",
        "        row = df_5m_subset.iloc[i]\n",
        "        current_price = row['Close']\n",
        "        actual_next = df_5m_subset.iloc[i + 1]['Close']\n",
        "\n",
        "        # 1. Update Bias\n",
        "        raw_pred_5m = row['Predicted_Raw']\n",
        "        current_bias = current_price - raw_pred_5m\n",
        "        bias_window.append(current_bias)\n",
        "        if len(bias_window) > BIAS_WINDOW_SIZE:\n",
        "            bias_window.pop(0)\n",
        "        avg_bias = sum(bias_window) / len(bias_window)\n",
        "\n",
        "        pred_5m_corrected = raw_pred_5m + avg_bias\n",
        "\n",
        "        # 2. Big Brother Trend\n",
        "        hour_key = row['Date'].floor('H')\n",
        "        trend = \"NEUTRAL\"\n",
        "\n",
        "        if hour_key in hourly_trend:\n",
        "            h_data = hourly_trend[hour_key]\n",
        "            pred_bullish = (h_data['Predicted_Raw'] - h_data['Close']) > -avg_bias\n",
        "            ema_bullish = (h_data['Close'] > h_data['EMA_50']) and (h_data['EMA_20'] > h_data['EMA_50'])\n",
        "            ema_bearish = (h_data['Close'] < h_data['EMA_50']) and (h_data['EMA_20'] < h_data['EMA_50'])\n",
        "\n",
        "            if pred_bullish and ema_bullish:\n",
        "                trend = \"BULLISH\"\n",
        "            elif not pred_bullish and ema_bearish:\n",
        "                trend = \"BEARISH\"\n",
        "\n",
        "        # 3. Little Brother Entry\n",
        "        pred_change = (pred_5m_corrected - current_price) / current_price\n",
        "\n",
        "        entry_signal = \"WAIT\"\n",
        "        if pred_change > MIN_THRESHOLD:\n",
        "            entry_signal = \"BUY\"\n",
        "        elif pred_change < -MIN_THRESHOLD:\n",
        "            entry_signal = \"SELL\"\n",
        "\n",
        "        # 4. RSI Filter\n",
        "        if entry_signal == \"BUY\" and row['RSI'] > RSI_BUY_MAX:\n",
        "            entry_signal = \"WAIT\"\n",
        "            results['filtered_signals'] += 1\n",
        "        if entry_signal == \"SELL\" and row['RSI'] < RSI_SELL_MIN:\n",
        "            entry_signal = \"WAIT\"\n",
        "            results['filtered_signals'] += 1\n",
        "\n",
        "        # === FINAL DECISION ===\n",
        "        results['total_signals'] += 1\n",
        "\n",
        "        if trend == \"BULLISH\" and entry_signal == \"BUY\":\n",
        "            results['buy_signals'] += 1\n",
        "            results['confirmed_trades'] += 1\n",
        "            if actual_next > current_price:\n",
        "                results['wins'] += 1\n",
        "            else:\n",
        "                results['losses'] += 1\n",
        "            trades.append((actual_next - current_price) / current_price)\n",
        "\n",
        "        elif trend == \"BEARISH\" and entry_signal == \"SELL\":\n",
        "            results['sell_signals'] += 1\n",
        "            results['confirmed_trades'] += 1\n",
        "            if actual_next < current_price:\n",
        "                results['wins'] += 1\n",
        "            else:\n",
        "                results['losses'] += 1\n",
        "            trades.append((current_price - actual_next) / current_price)\n",
        "\n",
        "    # Metrics\n",
        "    if results['confirmed_trades'] > 0:\n",
        "        results['win_rate'] = (results['wins'] / results['confirmed_trades']) * 100\n",
        "    else:\n",
        "        results['win_rate'] = 0\n",
        "\n",
        "    return results, trades\n",
        "\n",
        "# =============\n",
        "# MAIN\n",
        "# =============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  2025 STOCK PREDICTION & ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    def check_model_exists(name):\n",
        "        if os.path.exists(name): return True\n",
        "        return False\n",
        "\n",
        "    if not check_model_exists(BIG_BROTHER_MODEL) or not check_model_exists(LITTLE_BROTHER_MODEL):\n",
        "        print(\"CRITICAL ERROR: Models not found in current directory.\")\n",
        "        print(f\"Looking for: {BIG_BROTHER_MODEL} and {LITTLE_BROTHER_MODEL}\")\n",
        "        print(\"Please upload the .keras model files.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load artifacts\n",
        "    print(\"Loading models and scalers...\")\n",
        "    try:\n",
        "        big_model = load_model(BIG_BROTHER_MODEL)\n",
        "        little_model = load_model(LITTLE_BROTHER_MODEL)\n",
        "\n",
        "        with open(SCALER_SMALL, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        with open(CLOSE_SCALER, 'rb') as f:\n",
        "            close_scaler = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models/scalers: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load 2025 Data\n",
        "    print(\"\\nLoading Data for 2025...\")\n",
        "    df_5m = load_and_preprocess(TEST_5M_FILENAME, filter_year=2025)\n",
        "    df_1h = load_and_preprocess(TEST_1H_FILENAME, filter_year=2025)\n",
        "\n",
        "    if df_5m is None or df_5m.empty or df_1h is None or df_1h.empty:\n",
        "        print(\"\\nCRITICAL ERROR: Could not proceed due to missing or empty data.\")\n",
        "        print(\"Please check that TEST_5m.csv and TEST_1h.csv are uploaded.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Predictions\n",
        "    print(\"\\nGenerating Predictions...\")\n",
        "\n",
        "    # 5M\n",
        "    df_5m_scaled = df_5m.copy()\n",
        "    df_5m_scaled[FEATURE_COLS] = scaler.transform(df_5m[FEATURE_COLS])\n",
        "    predict_5m = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=df_5m_scaled[FEATURE_COLS].values, targets=None,\n",
        "        sequence_length=LOOKBACK_SMALL, batch_size=256, shuffle=False\n",
        "    )\n",
        "    pred_5m_scaled = little_model.predict(predict_5m, verbose=0)\n",
        "    little_predictions = close_scaler.inverse_transform(pred_5m_scaled).flatten()\n",
        "\n",
        "    # 1H\n",
        "    df_1h_scaled = df_1h.copy()\n",
        "    df_1h_scaled[FEATURE_COLS] = scaler.transform(df_1h[FEATURE_COLS])\n",
        "    predict_1h = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=df_1h_scaled[FEATURE_COLS].values, targets=None,\n",
        "        sequence_length=LOOKBACK_BIG, batch_size=64, shuffle=False\n",
        "    )\n",
        "    pred_1h_scaled = big_model.predict(predict_1h, verbose=0)\n",
        "    big_predictions = close_scaler.inverse_transform(pred_1h_scaled).flatten()\n",
        "\n",
        "    # Run Compare\n",
        "    results, trades = hybrid_backtest(\n",
        "        df_5m, little_predictions,\n",
        "        df_1h, big_predictions,\n",
        "        LOOKBACK_SMALL, LOOKBACK_BIG\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  2025 PREDICTION RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Total Opportunities Scanned: {results['total_signals']}\")\n",
        "    print(f\"  Trades Taken (Agreement):    {results['confirmed_trades']}\")\n",
        "    print(f\"  Win Rate:                    {results['win_rate']:.2f}%\")\n",
        "    print(f\"  Wins:                        {results['wins']}\")\n",
        "    print(f\"  Losses:                      {results['losses']}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if len(trades) > 0:\n",
        "        total_return_pct = (np.prod([1+t for t in trades]) - 1) * 100\n",
        "        print(f\"  Estimated Return:            {total_return_pct:.2f}% (Compounded)\")\n",
        "\n",
        "    # =============\n",
        "    # VISUALIZATION\n",
        "    # =============\n",
        "    print(\"\\nGenerating Graph...\")\n",
        "    start_idx = LOOKBACK_SMALL - 1\n",
        "    end_idx = start_idx + len(little_predictions)\n",
        "\n",
        "    plot_dates = df_5m['Date'].iloc[start_idx:end_idx]\n",
        "    real_price = df_5m['Close'].iloc[start_idx:end_idx]\n",
        "\n",
        "    # Create Figure\n",
        "    fig = make_subplots(rows=1, cols=1, subplot_titles=(\"2025: Real Market vs AI Prediction\"))\n",
        "\n",
        "    # 1. Real Data (Green)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=plot_dates,\n",
        "        y=real_price,\n",
        "        mode='lines',\n",
        "        name='Real Price (Market)',\n",
        "        line=dict(color='#00FF00', width=1)\n",
        "    ))\n",
        "\n",
        "    # 2. AI Prediction (Red)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=plot_dates,\n",
        "        y=little_predictions,\n",
        "        mode='lines',\n",
        "        name='AI Prediction',\n",
        "        line=dict(color='#FF0000', width=1, dash='dot')\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        template='plotly_dark',\n",
        "        title_text='AI Model Performance: Real (Green) vs Predicted (Red)',\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Price',\n",
        "        height=600,\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Show figure\n",
        "    print(\"Graph generated. Displaying...\")\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "82l9Qtr6Wgwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST DEMO TO PREDICT THE 2026 XAU/USD MARKET MOVEMENT"
      ],
      "metadata": {
        "id": "hsDWEqw-W7mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============\n",
        "# SIMULATE 2026 WEALTH (Monte Carlo Projection)\n",
        "# =============\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# =============\n",
        "# CONFIGURATION\n",
        "# =============\n",
        "BIG_BROTHER_MODEL = 'big_brother_v2.keras'\n",
        "LITTLE_BROTHER_MODEL = 'little_brother_v2.keras'\n",
        "SCALER_SMALL = 'scaler_small.pkl'\n",
        "CLOSE_SCALER = 'close_scaler.pkl'\n",
        "\n",
        "# Filenames to check\n",
        "TEST_5M_FILENAME = 'TEST_5m.csv'\n",
        "TEST_1H_FILENAME = 'TEST_1h.csv'\n",
        "\n",
        "# BACKTEST SETTINGS\n",
        "LOOKBACK_BIG = 168\n",
        "LOOKBACK_SMALL = 48\n",
        "INITIAL_CAPITAL = 200.0  # User Request\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'Returns', 'EMA_20', 'EMA_50', 'EMA_Diff',\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
        "    'BB_Width', 'BB_Position', 'ATR', 'Volatility'\n",
        "]\n",
        "\n",
        "# =============\n",
        "# HELPER FUNCTIONS\n",
        "# =============\n",
        "# ... (Technical indicators reduced for brevity, assuming data has them or we calculate same as before)\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd - signal_line\n",
        "    return macd, signal_line, histogram\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
        "    middle = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "    upper = middle + (std * std_dev)\n",
        "    lower = middle - (std * std_dev)\n",
        "    return upper, middle, lower\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    df['EMA_Diff'] = df['EMA_20'] - df['EMA_50']\n",
        "    df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['Close'])\n",
        "    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df['Close'])\n",
        "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'])\n",
        "    df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "    return df\n",
        "\n",
        "def find_file(filename):\n",
        "    path1 = os.path.join('DATA_SET', filename)\n",
        "    if os.path.exists(path1): return path1\n",
        "    if os.path.exists(filename): return filename\n",
        "    return None\n",
        "\n",
        "def load_and_preprocess(filename, filter_year=None):\n",
        "    filepath = find_file(filename)\n",
        "    if not filepath:\n",
        "        print(f\"Skipping {filename} (not found)\")\n",
        "        return None\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    df = pd.read_csv(filepath, sep=';')\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d %H:%M')\n",
        "\n",
        "    if filter_year:\n",
        "        df_filtered = df[df['Date'].dt.year == filter_year]\n",
        "        if not df_filtered.empty:\n",
        "            df = df_filtered\n",
        "\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    df = add_technical_indicators(df)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    df[float_cols] = df[float_cols].astype('float32')\n",
        "    return df\n",
        "\n",
        "def hybrid_backtest(df_5m, little_predictions, df_1h, big_predictions):\n",
        "    # Returns list of trade pct returns\n",
        "    trades = []\n",
        "\n",
        "    # Simple filters (matching predict_2025.py)\n",
        "    RSI_BUY_MAX = 70\n",
        "    RSI_SELL_MIN = 30\n",
        "    MIN_THRESHOLD = 0.0015\n",
        "    BIAS_WINDOW_SIZE = 50\n",
        "    bias_window = []\n",
        "\n",
        "    # Align\n",
        "    df_5m_subset = df_5m.iloc[LOOKBACK_SMALL-1 : LOOKBACK_SMALL-1+len(little_predictions)].copy()\n",
        "    df_5m_subset['Predicted_Raw'] = little_predictions\n",
        "\n",
        "    df_1h_subset = df_1h.iloc[LOOKBACK_BIG-1 : LOOKBACK_BIG-1+len(big_predictions)].copy()\n",
        "    df_1h_subset['Predicted_Raw'] = big_predictions\n",
        "    df_1h_subset['Hour'] = df_1h_subset['Date'].dt.floor('H')\n",
        "    hourly_trend = df_1h_subset.set_index('Hour')[['Close', 'Predicted_Raw', 'EMA_20', 'EMA_50']].to_dict('index')\n",
        "\n",
        "    for i in range(len(df_5m_subset) - 1):\n",
        "        row = df_5m_subset.iloc[i]\n",
        "        current_price = row['Close']\n",
        "        actual_next = df_5m_subset.iloc[i + 1]['Close']\n",
        "\n",
        "        # Bias\n",
        "        raw_pred_5m = row['Predicted_Raw']\n",
        "        current_bias = current_price - raw_pred_5m\n",
        "        bias_window.append(current_bias)\n",
        "        if len(bias_window) > BIAS_WINDOW_SIZE: bias_window.pop(0)\n",
        "        avg_bias = sum(bias_window) / len(bias_window)\n",
        "        pred_5m_corrected = raw_pred_5m + avg_bias\n",
        "\n",
        "        # Trend\n",
        "        hour_key = row['Date'].floor('H')\n",
        "        trend = \"NEUTRAL\"\n",
        "        if hour_key in hourly_trend:\n",
        "            h_data = hourly_trend[hour_key]\n",
        "            pred_bullish = (h_data['Predicted_Raw'] - h_data['Close']) > -avg_bias\n",
        "            ema_bullish = (h_data['Close'] > h_data['EMA_50']) and (h_data['EMA_20'] > h_data['EMA_50'])\n",
        "            ema_bearish = (h_data['Close'] < h_data['EMA_50']) and (h_data['EMA_20'] < h_data['EMA_50'])\n",
        "            if pred_bullish and ema_bullish: trend = \"BULLISH\"\n",
        "            elif not pred_bullish and ema_bearish: trend = \"BEARISH\"\n",
        "\n",
        "        # Entry\n",
        "        pred_change = (pred_5m_corrected - current_price) / current_price\n",
        "        entry_signal = \"WAIT\"\n",
        "        if pred_change > MIN_THRESHOLD: entry_signal = \"BUY\"\n",
        "        elif pred_change < -MIN_THRESHOLD: entry_signal = \"SELL\"\n",
        "\n",
        "        # RSI\n",
        "        if entry_signal == \"BUY\" and row['RSI'] > RSI_BUY_MAX: entry_signal = \"WAIT\"\n",
        "        if entry_signal == \"SELL\" and row['RSI'] < RSI_SELL_MIN: entry_signal = \"WAIT\"\n",
        "\n",
        "        # Execute\n",
        "        if trend == \"BULLISH\" and entry_signal == \"BUY\":\n",
        "            trades.append((actual_next - current_price) / current_price)\n",
        "        elif trend == \"BEARISH\" and entry_signal == \"SELL\":\n",
        "            trades.append((current_price - actual_next) / current_price)\n",
        "\n",
        "    return trades\n",
        "\n",
        "def monte_carlo_simulation(trades_history, num_simulations=50, years=2, initial_balance=2000.0)\n",
        "\n",
        "    trades_per_year = len(trades_history)\n",
        "    total_trades_future = trades_per_year * years\n",
        "\n",
        "    simulation_results = []\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        balance = initial_balance\n",
        "        equity_curve = [balance]\n",
        "\n",
        "        for _ in range(total_trades_future):\n",
        "            r_trade = random.choice(trades_history)\n",
        "\n",
        "            balance = balance * (1 + r_trade)\n",
        "            equity_curve.append(balance)\n",
        "\n",
        "        simulation_results.append(equity_curve)\n",
        "\n",
        "    return simulation_results, total_trades_future\n",
        "\n",
        "# =============\n",
        "# MAIN\n",
        "# =============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  WEALTH SIMULATION (2026)\")\n",
        "    print(f\"  Starting Capital: ${INITIAL_CAPITAL}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Load Models & Data (2025 Baseline)\n",
        "    try:\n",
        "        big_model = load_model(BIG_BROTHER_MODEL)\n",
        "        little_model = load_model(LITTLE_BROTHER_MODEL)\n",
        "        with open(SCALER_SMALL, 'rb') as f: scaler = pickle.load(f)\n",
        "        with open(CLOSE_SCALER, 'rb') as f: close_scaler = pickle.load(f)\n",
        "    except:\n",
        "        print(\"Error loading models. Make sure .keras and .pkl files are present.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df_5m = load_and_preprocess(TEST_5M_FILENAME)\n",
        "    df_1h = load_and_preprocess(TEST_1H_FILENAME)\n",
        "\n",
        "    if df_5m is None or df_1h is None:\n",
        "        print(\"Data missing. Cannot benchmark.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 2. Run Baseline Backtest\n",
        "    print(\"\\nBenchmarking on Test Data to map probability...\")\n",
        "\n",
        "    # Predict 5M\n",
        "    df_5m_s = df_5m.copy()\n",
        "    df_5m_s[FEATURE_COLS] = scaler.transform(df_5m[FEATURE_COLS])\n",
        "    p_5m = little_model.predict(tf.keras.utils.timeseries_dataset_from_array(\n",
        "        df_5m_s[FEATURE_COLS].values, None, LOOKBACK_SMALL, batch_size=256, shuffle=False\n",
        "    ), verbose=0)\n",
        "    pred_5m = close_scaler.inverse_transform(p_5m).flatten()\n",
        "\n",
        "    # Predict 1H\n",
        "    df_1h_s = df_1h.copy()\n",
        "    df_1h_s[FEATURE_COLS] = scaler.transform(df_1h[FEATURE_COLS])\n",
        "    p_1h = big_model.predict(tf.keras.utils.timeseries_dataset_from_array(\n",
        "        df_1h_s[FEATURE_COLS].values, None, LOOKBACK_BIG, batch_size=64, shuffle=False\n",
        "    ), verbose=0)\n",
        "    pred_1h = close_scaler.inverse_transform(p_1h).flatten()\n",
        "\n",
        "    # Extract Trades\n",
        "    historical_trades = hybrid_backtest(df_5m, pred_5m, df_1h, pred_1h)\n",
        "\n",
        "    if not historical_trades:\n",
        "        print(\"No trades found in baseline data. Cannot simulate.\")\n",
        "        sys.exit()\n",
        "\n",
        "    win_rate = len([x for x in historical_trades if x > 0]) / len(historical_trades)\n",
        "    avg_return = np.mean(historical_trades) * 100\n",
        "    print(f\"  Baseline Metrics:\")\n",
        "    print(f\"  - Win Rate: {win_rate*100:.1f}%\")\n",
        "    print(f\"  - Avg Return/Trade: {avg_return:.2f}%\")\n",
        "    print(f\"  - Total Samples: {len(historical_trades)}\")\n",
        "\n",
        "    # 3. Simulate Future (Monte Carlo)\n",
        "    print(\"\\nSimulating 2026 (Monte Carlo Projection)...\")\n",
        "    sim_curves, num_future_trades = monte_carlo_simulation(historical_trades, num_simulations=50, years=1, initial_balance=INITIAL_CAPITAL)\n",
        "\n",
        "    # Calculate Percentiles\n",
        "    # Transpose to get distribution at each step\n",
        "    steps = len(sim_curves[0])\n",
        "    median_curve = []\n",
        "    p90_curve = []\n",
        "    p10_curve = []\n",
        "\n",
        "    for i in range(steps):\n",
        "        step_vals = [curve[i] for curve in sim_curves]\n",
        "        median_curve.append(np.median(step_vals))\n",
        "        p90_curve.append(np.percentile(step_vals, 90))\n",
        "        p10_curve.append(np.percentile(step_vals, 10))\n",
        "\n",
        "    final_median = median_curve[-1]\n",
        "    final_p90 = p90_curve[-1]\n",
        "    final_p10 = p10_curve[-1]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  SIMULATION RESULTS (End of 2026)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Starting Balance:   ${INITIAL_CAPITAL:,.2f}\")\n",
        "    print(f\"  Projected Median:   ${final_median:,.2f} (Likely)\")\n",
        "    print(f\"  Optimistic (90%):   ${final_p90:,.2f} (Lucky Streak)\")\n",
        "    print(f\"  Pessimistic (10%):  ${final_p10:,.2f} (Bad Streak)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 4. Visualization\n",
        "    fig = make_subplots(rows=1, cols=1, subplot_titles=(f\"Wealth Projection 2026 (Start: ${INITIAL_CAPITAL})\"))\n",
        "\n",
        "    # Plot all simulation traces (faint)\n",
        "    x_axis = list(range(steps))\n",
        "    for curve in sim_curves[:20]: # Show first 20 lines\n",
        "        fig.add_trace(go.Scatter(x=x_axis, y=curve, mode='lines', line=dict(color='rgba(255, 255, 255, 0.1)'), showlegend=False))\n",
        "\n",
        "    # Plot Median (Green - \"Real\" expectation)\n",
        "    fig.add_trace(go.Scatter(x=x_axis, y=median_curve, mode='lines', name='Expected Growth (Median)', line=dict(color='#00FF00', width=3)))\n",
        "\n",
        "    # Plot Bands (Red - \"Risk/Variance\")\n",
        "    fig.add_trace(go.Scatter(x=x_axis, y=p90_curve, mode='lines', name='Optimistic Top', line=dict(color='#FF0000', dash='dot')))\n",
        "    fig.add_trace(go.Scatter(x=x_axis, y=p10_curve, mode='lines', name='Pessimistic Bottom', line=dict(color='#FF0000', dash='dot')))\n",
        "\n",
        "    fig.update_layout(\n",
        "        template='plotly_dark',\n",
        "        title='Projected Account Balance (2026)',\n",
        "        xaxis_title='Trade Count (Approx 1 Year)',\n",
        "        yaxis_title='Balance ($)',\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    print(\"Displaying Graph...\")\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "vFLH8TsZW7Ns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}